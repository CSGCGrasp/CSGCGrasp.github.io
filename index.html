<!DOCTYPE html>
<html>
    <head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover" name="viewport" />
    <link rel="alternate icon" type="image/png" href="/img/favicon.ico">
    <title>KARNet</title>
    
<link rel="stylesheet" href="/css/reset.css">

    
<link rel="stylesheet" href="/css/style.css">

    
<link rel="stylesheet" href="/css/markdown.css">

    
<link rel="stylesheet" href="/css/fonts.css">


    <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.css">
<meta name="generator" content="Hexo 6.3.0"></head>
    <body>
        <div class="paper">
            <div class="paper-main">

                <div class="post-list">
    
    <div class="post">
        <!-- <a class="post-title" href="/2024/03/12/paper/">Infusing Multi-Source Heterogeneous Knowledge for Language-Conditional Segmentation and Grasping</a> -->
        <div class="post-title" >Infusing Multi-Source Heterogeneous Knowledge for Language-Conditional Segmentation and Grasping</div>
        <div class="post-authors" >Jialong Xie, Jin Liu, Zhenwei Zhu, Chaoqun Wang, Peng Duan, and  Fengyu Zhou</div>
        <div class="post-affiliations" >Shandong University, Liaocheng University</div>
        <div class="head-icon"> 

            
                <a class="icon-container" target="_blank" rel="noopener" href="https://github.com/KARNetGrasp/KARNetGrasp.git">
                    <i class="fa fa-github-square"></i>
                </a>
            

            
                <a class="icon-container" target="_blank" rel="noopener" href="https://youtu.be/cmx21_H4CfY">
                    <i class="fa fa-youtube-square"></i>
                </a>
            
            
        </div>
        <div class="line"></div>
        <div class="post-abstract-title">Abstract</div>
        <div class="post-abstract" >Language-conditioned segmentation and grasping requires the robot to simultaneously identify and grasp a specific object in accordance with human linguistic instruction. Existing methods generally involve semantic matching between the entire instruction and raw RGB images to ground the desired object, generating a grasp pose for that object. However, they overlook the semantic effectiveness of the nouns in the instruction, struggling with fine-grained object localization. Furthermore, they lack sufficient geometry context of the object to reason the optimal grasp pose, resulting in instability and failures in cluttered environments. In this paper, we propose a Knowledge-Augmented Refinement network (KARNet) to jointly conduct fine-grained object segmentation and grasping detection to tackle these challenges. Specifically, to mine the semantic context of the nouns, we introduce an entity semantic enhancement module to fuse the knowledge from both the external knowledge base and the CLIP. Besides, a refinement decoder is proposed to generate segmentation masks and incorporate the geometry-aware features to yield suitable grasp poses for the desired objects. Notably, to achieve better context fusion between linguistics and vision, we further introduce a language-guided object parsing (LGOP) module to conduct coarse-to-fine multi-modal fusion. We conduct extensive experiments on the cluttered household dataset and demonstrate that our proposed approach attains a grasping accuracy of 97.97% and segmentation OIoU of 94.35%, reaching state-of-the-art performance. The effectiveness of our method is further validated in real-world applications.</div>
        <div class="post-video-title">
            <i class="fa fa-hand-o-right"></i><a  class= 'post-video-title' target="_blank" rel="noopener" href="https://youtu.be/cmx21_H4CfY">Video on YouTube</a><i class="fa fa-hand-o-left"></i>
            
        </div>
        
            <div class="video-container">
                <iframe
                width="560"
                height="315"
                src="https://www.youtube.com/embed/cmx21_H4CfY"
                frameborder="0"
                allowfullscreen
                ></iframe>
            </div>
        
        <div class="post-except">
            <h2 id="An-overview-of-the-proposed-knowledge-driven-robotic-grasping-framework"><a href="#An-overview-of-the-proposed-knowledge-driven-robotic-grasping-framework" class="headerlink" title="An overview of the proposed knowledge-driven robotic grasping framework"></a>An overview of the proposed knowledge-driven robotic grasping framework</h2><p>An overview of the proposed knowledge-driven robotic grasping framework. This pipeline mimics the three stages of the cognitive process for humans. The first stage is to apply the image-text knowledge of CLIP. The second stage is to enrich the semantics of the sentence through external knowledge continuously. The third stage is to enhance multi-modal fusion and grasping ability by geometry-aware cues.<br><img src="/images/pipeline.jpg" alt="knowledge-driven robotic grasping framework"></p>
<h2 id="Overview-of-KARNet"><a href="#Overview-of-KARNet" class="headerlink" title="Overview of KARNet"></a>Overview of KARNet</h2><p>An overview of KARNet architecture. Given a scene and language description of the target, our pipeline first fine-tunes the vision-language knowledge of CLIP to simultaneously understand visual and linguistic concepts. Second,  it queries the detailed description of entities from the external knowledge base, e.g., GPT-4, to generate semantic prototypes and inject them. Third, it leverages the U-shaped structure and LGOP module to parse the object-specific features. Lastly, we utilize geometry-aware features (e.g., boundary) to facilitate the refinement of grasp poses.<br><img src="/images/architecture.jpg" alt="Overview of KARNet"></p>
<h2 id="Qualitative-results"><a href="#Qualitative-results" class="headerlink" title="Qualitative results"></a>Qualitative results</h2><p>Qualitative results of our proposed method.<br><img src="/images/resultVisualization.jpg" alt="Visualization of our method"></p>
<p>Comparison with other methods.<br><img src="/images/compareresult.jpg" alt="Visualization of comparison"></p>
<h2 id="Description-of-different-knowledge-bases"><a href="#Description-of-different-knowledge-bases" class="headerlink" title="Description of different knowledge bases"></a>Description of different knowledge bases</h2><p><img src="/images/knowledgebase.jpg" alt="Description of different knowledge bases"></p>

            <!-- <a class="read-more" href="/2024/03/12/paper/"> ... </a> -->
        </div>
        <div class="post-date">2024.03.12</div>
    </div>
    
</div>

                <div class="footer">
    <span>Copyright Â© 2023 KARNet</span>
    <span>Designed By <a target="_blank" href="https://github.com/xiejialong/CTNet/tree/web">Jialong Xie</a></span>
</div>


<link rel="stylesheet" href="/css/a11y-dark.min.css">


<script src="/js/highlight.min.js"></script>


<script src="/js/highlightjs-line-numbers.js"></script>


<script>
    hljs.initHighlightingOnLoad();
    hljs.initLineNumbersOnLoad();
</script>

            </div>
        </div>
    </body>
</html>