<!DOCTYPE html>
<html>
    <head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover" name="viewport" />
    <link rel="alternate icon" type="image/png" href="/img/favicon.ico">
    <title>CSGC</title>
    
<link rel="stylesheet" href="/css/reset.css">

    
<link rel="stylesheet" href="/css/style.css">

    
<link rel="stylesheet" href="/css/markdown.css">

    
<link rel="stylesheet" href="/css/fonts.css">

    <!-- 
<link rel="stylesheet" href="/css/font-awesome.min.css">
 -->
    <script src="https://kit.fontawesome.com/6ac35365cd.js" crossorigin="anonymous"></script>
    <!-- <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.css"> -->
<meta name="generator" content="Hexo 6.3.0"></head>
    <body>
        <div class="paper">
            <div class="paper-main">

                <div class="post-list">
    
    <div class="post">
        <!-- <a class="post-title" href="/2024/07/19/paper/">Semi-Supervised Language-Conditioned Grasping with Curriculum-Scheduled Augmentation and Geometric Consistency</a> -->
        <div class="post-title" >Semi-Supervised Language-Conditioned Grasping with Curriculum-Scheduled Augmentation and Geometric Consistency</div>
        <div class="post-authors" >Jialong Xie, Fengyu Zhou, Jin Liu, and Chaoqun Wang</div>
        <div class="post-affiliations" >Shandong University</div>
        <div class="head-icon"> 

            
   
            
                <a class="icon-container" target="_blank" rel="noopener" href="https://youtu.be/iKC78uq1NxU" title="youtube">
                    <i class="fa-brands fa-square-youtube"></i>
                </a>
            

            
                <a class="icon-container" target="_blank" rel="noopener" href="https://drive.google.com/drive/folders/1_4buRsO26159XgCH2uPt4VTMq5Dz0q_u?usp=drive_link" title="dataset">
                    <i class="fa-brands fa-google-drive"></i>
                </a>
            

        </div>
        <div class="line"></div>
        <div class="post-abstract-title">Abstract</div>
        <div class="post-abstract" >Language-conditioned grasping (LCG) is an essential skill for robotic manipulation and has recently attracted increasing interest. However, existing methods often need numerous paired image-text-pose annotations for fully supervised learning, which are tedious and expensive. In this paper, we first propose a semi-supervised language-conditioned grasping framework that achieves data-efficient object grounding and grasping detection according to the language description. Recent studies on semi-supervised learning have made progress in vision tasks, e.g., semantic segmentation. However, these methods suffer from over-distorted data perturbations, resulting in slow and unstable convergence for multi-modal inputs in the early stage. Moreover, they do not consider the consistency between perceptive and grasping location, leading to hand-eye inconsistency and low grasp accuracy. Therefore, we introduce a Curriculum-Scheduled augmentation and Geometric Consistency (CSGC) strategy to overcome above problem. Concretely, We design a curriculum-scheduled augmentation strategy to progressively improve data diversity from easy to difficult, facilitating stable knowledge distillation and model's convergence. Meanwhile, we present a geometry-aware consistency regularization to constrain the region alignment between object perception and grasping confidence, improving the quality of pseudo-labels and grasp accuracy. Extensive experimental results demonstrate the effectiveness and practicability of our proposed method in the limited labeled data.</div>
        <div class="post-video-title">
            <i class="fa fa-hand-o-right"></i><a  class= 'post-video-title' target="_blank" rel="noopener" href="https://youtu.be/iKC78uq1NxU">Video on YouTube</a><i class="fa fa-hand-o-left"></i>
            
        </div>
        
            <div class="video-container">
                <iframe
                width="560"
                height="315"
                src="https://www.youtube.com/embed/iKC78uq1NxU"
                frameborder="0"
                allowfullscreen
                ></iframe>
            </div>
        
        <div class="post-except">
            <h2 id="An-overview-of-the-proposed-knowledge-driven-robotic-grasping-framework"><a href="#An-overview-of-the-proposed-knowledge-driven-robotic-grasping-framework" class="headerlink" title="An overview of the proposed knowledge-driven robotic grasping framework"></a>An overview of the proposed knowledge-driven robotic grasping framework</h2><p>An overview of the proposed knowledge-driven robotic grasping framework. This pipeline mimics the three stages of the cognitive process for humans. The first stage is to apply the image-text knowledge of CLIP. The second stage is to enrich the semantics of the sentence through external knowledge continuously. The third stage is to enhance multi-modal fusion and grasping ability by geometry-aware cues.<br><img src="/images/scheme.jpg" alt="knowledge-driven robotic grasping framework"></p>
<h2 id="Overview-of-KARNet"><a href="#Overview-of-KARNet" class="headerlink" title="Overview of KARNet"></a>Overview of KARNet</h2><p>An overview of KARNet architecture. Given a scene and language description of the target, our pipeline first fine-tunes the vision-language knowledge of CLIP to simultaneously understand visual and linguistic concepts. Second,  it queries the detailed description of entities from the external knowledge base, e.g., GPT-4, to generate semantic prototypes and inject them. Third, it leverages the U-shaped structure and LGOP module to parse the object-specific features. Lastly, we utilize geometry-aware features (e.g., boundary) to facilitate the refinement of grasp poses.<br><img src="/images/OurMethod.jpg" alt="Overview of KARNet"></p>
<h2 id="Qualitative-results"><a href="#Qualitative-results" class="headerlink" title="Qualitative results"></a>Qualitative results</h2><p>Qualitative results of our proposed method.<br><img src="/images/compare_more.jpg" alt="Visualization of our method"></p>
<p>Comparison with other methods.<br><img src="/images/compareresult.jpg" alt="Visualization of comparison"></p>
<h2 id="Whole-process-of-CSGC"><a href="#Whole-process-of-CSGC" class="headerlink" title="Whole process of CSGC"></a>Whole process of CSGC</h2><p><img src="/images/semiSupervised.jpg" alt="Description of different knowledge bases"></p>
<h4 id="GIF-of-manipulation"><a href="#GIF-of-manipulation" class="headerlink" title="GIF of manipulation"></a>GIF of manipulation</h4><table rules="none" align="center">
    <tr>
        <td>
            <center>
                <img src="images/scene1.gif" width="60%" />
                <br/>
                <font color="AAAAAA">Scene 1</font>
            </center>
        </td>
        <td>
            <center>
                <img src="images/scene2.gif" width="60%" />
                <br/>
                <font color="AAAAAA">Scene 2</font>
            </center>
        </td>
    </tr>
</table>



            <!-- <a class="read-more" href="/2024/07/19/paper/"> ... </a> -->
        </div>
        <div class="post-date">2024.07.19</div>
    </div>
    
</div>

                <div class="footer">
    <span>Copyright Â© 2023 CSGC</span>
    <span>Designed By <a target="_blank" href="https://github.com/xiejialong/CTNet/tree/web">Jialong Xie</a></span>
</div>


<link rel="stylesheet" href="/css/a11y-dark.min.css">


<script src="/js/highlight.min.js"></script>


<script src="/js/highlightjs-line-numbers.js"></script>


<script>
    hljs.initHighlightingOnLoad();
    hljs.initLineNumbersOnLoad();
</script>

            </div>
        </div>
    </body>
</html>