<!DOCTYPE html>
<html>
    <head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover" name="viewport" />
    <link rel="alternate icon" type="image/png" href="/img/favicon.ico">
    <title>CSGC</title>
    
<link rel="stylesheet" href="/css/reset.css">

    
<link rel="stylesheet" href="/css/style.css">

    
<link rel="stylesheet" href="/css/markdown.css">

    
<link rel="stylesheet" href="/css/fonts.css">

    <!-- 
<link rel="stylesheet" href="/css/font-awesome.min.css">
 -->
    <script src="https://kit.fontawesome.com/6ac35365cd.js" crossorigin="anonymous"></script>
    <!-- <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.css"> -->
<meta name="generator" content="Hexo 6.3.0"></head>
    <body>
        <div class="paper">
            <div class="paper-main">

                <div class="post-main">

    
    
        <div class="post-main-title">
            Semi-Supervised Language-Conditioned Grasping with Curriculum-Scheduled Augmentation and Geometric Consistency
        </div>
        <div class="post-meta">
            2024-07-08
        </div>
        <div class="post-md">
            <h2 id="An-overview-of-the-proposed-knowledge-driven-robotic-grasping-framework"><a href="#An-overview-of-the-proposed-knowledge-driven-robotic-grasping-framework" class="headerlink" title="An overview of the proposed knowledge-driven robotic grasping framework"></a>An overview of the proposed knowledge-driven robotic grasping framework</h2><p>An overview of the proposed knowledge-driven robotic grasping framework. This pipeline mimics the three stages of the cognitive process for humans. The first stage is to apply the image-text knowledge of CLIP. The second stage is to enrich the semantics of the sentence through external knowledge continuously. The third stage is to enhance multi-modal fusion and grasping ability by geometry-aware cues.<br><img src="/images/scheme.jpg" alt="knowledge-driven robotic grasping framework"></p>
<h2 id="Overview-of-KARNet"><a href="#Overview-of-KARNet" class="headerlink" title="Overview of KARNet"></a>Overview of KARNet</h2><p>An overview of KARNet architecture. Given a scene and language description of the target, our pipeline first fine-tunes the vision-language knowledge of CLIP to simultaneously understand visual and linguistic concepts. Second,  it queries the detailed description of entities from the external knowledge base, e.g., GPT-4, to generate semantic prototypes and inject them. Third, it leverages the U-shaped structure and LGOP module to parse the object-specific features. Lastly, we utilize geometry-aware features (e.g., boundary) to facilitate the refinement of grasp poses.<br><img src="/images/OurMethod.jpg" alt="Overview of KARNet"></p>
<h2 id="Qualitative-results"><a href="#Qualitative-results" class="headerlink" title="Qualitative results"></a>Qualitative results</h2><p>Qualitative results of our proposed method.<br><img src="/images/compare_more.jpg" alt="Visualization of our method"></p>
<p>Comparison with other methods.<br><img src="/images/compareresult.jpg" alt="Visualization of comparison"></p>
<h2 id="Whole-process-of-CSGC"><a href="#Whole-process-of-CSGC" class="headerlink" title="Whole process of CSGC"></a>Whole process of CSGC</h2><p><img src="/images/semiSupervised.jpg" alt="Description of different knowledge bases"></p>

        </div>

    

</div>
                <div class="footer">
    <span>Copyright Â© 2023 CSGC</span>
    <span>Designed By <a target="_blank" href="https://github.com/xiejialong/CTNet/tree/web">Jialong Xie</a></span>
</div>


<link rel="stylesheet" href="/css/a11y-dark.min.css">


<script src="/js/highlight.min.js"></script>


<script src="/js/highlightjs-line-numbers.js"></script>


<script>
    hljs.initHighlightingOnLoad();
    hljs.initLineNumbersOnLoad();
</script>

            </div>
        </div>
    </body>
</html>